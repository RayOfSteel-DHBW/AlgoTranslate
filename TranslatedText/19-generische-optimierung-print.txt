Algorithms 1
Generic Optimization Methods

1 – The Research University in the Helmholtz Association
KIT

www.kit.edu

 Example: Balanced and Cheap
Problem
Burgers do not meet official nutritional guidelines
per dish missing 0.5 mg Vitamin A, 15 mg Vitamin C, 4 g fiber
Goal: fix this problem at the lowest possible cost
Pickles

Carrots

White cabbage

Vitamin A (mg/kg)

35

0.5

0.5

Vitamin C (mg/kg)

60

300

10

Fiber (g/kg)

30
0.75

20
0.5

10
0.15

Price (€/kg)

. . . and when Rabbid said, "Honey or condensed milk with your bread?" he was so
excited that he said, "Both," and then, so
as not to seem greedy, he added, "But
don't bother about the bread, please."
.
A. A. Milne, Winnie the Pooh

Solution: x1 ; x2 ; x3 represent the amount of carrots, cabbage and pickles
minimize: 0.75x1 + 0.5x2 + 0.15x3
optimal solution:
constraints: 35x1 + 0.5x2 + 0.5x3 ≥ 0.5
9.5 g carrots
60x1 + 300x2 + 10x3 ≥ 15
38 g cabbage
30x1 + 20x2 + 10x3 ≥ 4
xi ≥ 0
290 g pickles
2

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Linear Programs
Trivia
were already used in the 40s (and solved manually)
"Program" is a military term for various types of plans (e.g., supply plan, troop deployment plan, etc.)
first major LP solved with the simplex algorithm
optimize costs for balanced nutrition
77 variables, 9 constraints
simplex method (by hand in 1947): 120 person-days
somewhat later (using computer): George Dantzig tries to optimize his own nutrition
first attempt: several liters of vinegar per day
second attempt: 200 bouillon cubes per day
⇒ formulating a sensible LP is not always trivial
3

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 A Difficult Graph Problem
Problem: Independent Set
Let G = (V; E) be a graph. Find a largest possible node set I ⊆ V such that for
each edge {u; v } ∈ E at most one of the nodes u or v lies in I.
Intuition
the edges model conflicts
find largest possible conflict-free node set
Algorithmic situation
Independent Set is NP-hard
we know no algorithm with polynomial runtime
we assume that no such algorithm exists
one should still not lose all hope
possibly many instances are well-behaved → only sometimes slow
suboptimal solutions can possibly be found efficiently
4

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Independent Set as Linear Program
The variables
node v ∈ V → variable xv
restriction: xv ∈ {0; 1}
interpretation: xv = 1 ⇔ v ∈ I

Problem: Independent Set
Let G = (V; E) be a graph. Find a
largest possible node set I ⊆ V such
that for each edge {u; v } ∈ E at most
one of the nodes u or v lies in I.

Optimization function
goal: choose as many nodes as possible → set many variables to 1
P
maximize:
xv
v ∈V

Constraints
for each edge: at most one of the endpoints selected
for each edge {u; v } ∈ E one constraint: xu + xv ≤ 1
Note
requirement that xv takes integer values makes the problem hard
5

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 LPs and ILPs
Linear Programs (LP)
given: linear optimization function and linear constraints
find optimal real-valued variable assignment while satisfying the constraints
theory: solvable in polynomial time
practice: highly efficient implementations available (free and commercial)
Integer Linear Programs (ILP – Integer Linear Program)
find optimal integer variable assignment while satisfying the constraints
theory: NP-hard (presumably not solvable in polynomial time)
practice: mostly efficient implementations available (free and commercial)
powerful generic tool to solve even difficult problems

6

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Brute Force
Naive plan
enumerate all subsets I ∈ V
test whether I is an independent set
output largest independent set

Problem: Independent Set
Let G = (V; E) be a graph. Find a
largest possible node set I ⊆ V such
that for each edge {u; v } ∈ E at most
one of the nodes u or v lies in I.

Observation
edge {u; v } ∈ E → if u ∈ I and v ∈ I, then I is not an independent set
we could actually save all subsets where u ∈ I and v ∈ I
Smarter brute force: Branching
find a single decision and try all possible cases
here: for a node v ∈ V either v ∈
= I or v ∈ I
v∈
= I → delete v from the graph and solve the remaining instance
v ∈ I → neighbors of v are all not in I → delete N(v ) from the graph and solve
the remaining instance
7

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Branching: Example
v
v∈
=I

v

v ∈I

v

Observation
in the branch v ∈ I the graph became significantly simpler
the more neighbors v has, the more nodes are deleted in the branch v ∈ I
sensible heuristic: choose high-degree node v for branching
8

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Branch-and-Bound
How can we make brute force faster?
clever branching (just seen)
make "obvious" decisions → reduction rule
current branch "obviously" bad → cut branch (pruning)
Reduction rule (example)
deg(v ) ≤ 1 ⇒ there exists a maximum independent set I with v ∈ I
Pruning (example)
lower bound (global): we already know a solution of size 17
upper bound (situation in current branch):
have selected 14 nodes so far
remaining graph can be covered with 3 cliques
solution in this branch cannot be better than 17
9

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Interim Status
Exact solutions, possibly slow
Branch-and-Bound: framework for brute-force algorithms
to be filled in: branching, reduction rules, pruning with upper/lower bounds
filling in degrees of freedom is very problem-specific
Formulation as (I)LP or also NLP (non-linear program)
translation into an (I)LP is problem-specific and not always easy
actual algorithm: use heavily optimized library
Heuristic approaches: sacrifice optimality for runtime
we already know: Greedy (often works well; usually not optimal)
coming up: problem-independent metaheuristics
Wikipedia on metaheuristics:
While the field also features high-quality research, many of the publications have been of poor quality; flaws
include vagueness, lack of conceptual elaboration, poor experiments, and ignorance of previous literature.
10

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 General Optimization Problem
Problem: generic maximization problem
Given a function f : X → R. Find x ∈ X with f (x) ≥ f (y ) for all y ∈ X.
Example: Independent Set
Option 1: X completely independent of the actual problem
X = 2V is the power set of V (e.g., encoded as bit vector {0; 1}n )
ȷ
|x| if x is an independent set
f (x) =
0
otherwise
Option 2: X contains only valid subsets → problem-specific
X ⊆ 2V is the set of all
Problem: Independent Set
independent sets
Let G = (V; E) be a graph. Find a
f (x) = |x|
largest possible node set I ⊆ V such
that for each edge {u; v } ∈ E at most
one of the nodes u or v lies in I.
11

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Local Search
Basic approach
start with some solution x ∈ X
iterate as long as desired:
choose solution y ∈ X that is similar to x
if f (y ) > f (x) → replace x with y

this might be worth doing problem-specifically

Possible concrete implementation (very generic)
assumption: X = {0; 1}n
start: choose x ∈ X uniformly at random
repeat until x doesn't change k times in a row:
set y = x and change a random bit of y
if f (y ) > f (x) → replace x with y

12

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Dealing with Local Optima
Problem
local search easily gets stuck in local optima
Solution approaches
restart with different initial solution
also allow small deteriorations with a certain probability
allow larger change steps
Coming up: evolutionary algorithm
one possible approach to extend local search
implements the various approaches to avoid local optima
there are a variety of variants and related algorithms
13

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Evolutionary Algorithm
Basic approach
start with a set P1 = {x11 ; : : : ; xn1 } ⊂ X of solutions
iterate as long as desired:
i
generate new solutions Oi = {y1i ; : : : ; ym
} from the current solution set Pi
select new solution set Pi+1 = {x1i+1 ; : : : ; xni+1 } ⊂ Pi ∪ Oi based on f
Pi ∪ Oi

Pi

Variation

Pi+1

Selection

Terminology borrowed from evolution
Pi is the ith population, f is the fitness function
the xji are the parents, the yji the offspring
14

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Variation: Recombination and Mutation
Recombination
choose two (or more) solutions x1i ; x2i ∈ Pi
combine x1i and x2i in some way
Example: 01100101
11011000

Pi

01101000

Variation

Mutation
similar to local search
change a (e.g., through recombination generated) solution somewhat
Example:
01101000

Pi ∪ Oi

00101001

15

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Evolutionary Algorithms: Note
Local minima and comparison to local search
mutation → as with local search
larger population → basically starts local search at multiple locations
recombination → larger jumps possible (not just local changes)
nevertheless: we cannot guarantee that we find the optimum
Independent Set: Generic vs. problem-specific
reminder: two possible definitions for the solution set X
X = 2V is the power set of V
Generic
Pro: very easy to implement
every bit vector comes into consideration
Con: doesn't always work well
generic recombination and mutation possible
X ⊆ 2V is the set of all independent sets
Problem-specific
only some bit vectors are valid
Con: more work
Pro: usually works significantly better
recombination and mutation problem-specific
16

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Generic Optimization Methods
Linear Programming
powerful modeling language to express various other problems
even more powerful: integer variables
powerful libraries for solving → one doesn't have to build an algorithm oneself
Branch-and-Bound (Brute-Force)
general framework to cleverly try all solutions
problem-specific: branching, reduction rules, pruning with upper/lower bounds
slow in worst-case, but often good in practice
Metaheuristics: Local Search, Evolutionary Algorithms
very generic → usable for all problems, low work effort
no (hardly any) guarantees for quality and runtime
problem-specific adaptations → better quality and runtime
17

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 Next Week
Monday: Last exercise
Dynamic programs
Q&A session
Wednesday: Last lecture
Review
What have we learned?
How did the course go?
Outlook
What learning opportunities are there for exam preparation?
What advanced courses are available?
What do we research?

18

Thomas Bläsius – Algorithms 1

Institute for Theoretical Computer Science, Scalable Algorithms

 